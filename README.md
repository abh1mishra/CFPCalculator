## Embedded systems often do not have hardware support for floating point. A solution for this is to use a software floating point library. we can also create custom floating-point encodings for different purposes, which is something that is done quite often in deep learning. 

#### Installation Procedure
Make and run aha
update flex if you face any compilation error


Note
You can do addition, multiplication only.
Print to see the result eg. a=5.2, print a
Use display command to see swfp representation which is its binary encoding.
swfp is the encoding of the floating point numbers.
