## Embedded systems often do not have hardware support for floating point. A solution for this is to use a software floating point library. we can also create custom floating-point encodings for different purposes, which is something that is done quite often in deep learning. 

#### Installation Procedure
Make and run aha <br />
update flex if you face any compilation error<br />


Note<br />
You can do addition, multiplication only.<br />
Print to see the result eg. a=5.2, print a<br />
Use display command to see swfp representation which is its binary encoding.<br />
swfp is the encoding of the floating point numbers.
